---
title: "ISYE 6501 Homework 5"
output: pdf_document
date: "2025-02-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
uscrime <- read.table("uscrime.txt", header = TRUE, sep = "\t")
library(outliers)
```
# Question 8.1
At my job I work closely with college or university student data. A situation where a linear regression model would be appropriate could be predicting a students grade for a course. Some predictors that could be used could be:

1. The student's average past course grade
1. The student's number of completed credit hours
1. The average course grade at the college or university where the course is taken
1. The student to instructor ratio at the college or university where the course is taken
1. The student's attendance in the course

---

# Question 8.2

### Perform grubs test to check for outliers
```{r}
# Check min value
grubbs.test(uscrime$Crime)

# Check max value
grubbs.test(uscrime$Crime, opposite = TRUE)

# Check min/max value
grubbs.test(uscrime$Crime, type = 11)
```
As you can see above, the p-value for all 3 tests is greater than 0.05, and therefore are not considered to be significant outliers. We fail to reject all null hypotheses and cannot accept any alternative hypotheses.

### Build and display the regression model for the uscrime dataset
```{r}
crime_model <- lm(uscrime$Crime ~ ., data = uscrime)

summary(crime_model)
```

### Predict crime for a new city
```{r}
new_city <- data.frame(
  M = 14.0, So = 0, Ed = 10.0, Po1 = 12.0, Po2 = 15.5, LF = 0.640,
  M.F = 94.0, Pop = 150, NW = 1.1, U1 = 0.120, U2 = 3.6, 
  Wealth = 3200, Ineq = 20.1, Prob = 0.04, Time = 39.0
)

predicted_crime <- predict(crime_model, newdata = new_city)

cat("Predicted Crime:", predicted_crime, "\n")
cat("Actual Crime Range:", range(uscrime$Crime))
```
This value, 155.4349, is the predicted value for the new city. However, this value appears to be unrealistic because it falls well outside of the range in the data. This is because it uses all predictors of the uscrime dataset. We need to remove predictors in order to get a better estimate.

### Look at p-values
```{r}
summary(crime_model)$coefficients[, 4]
```
Above, you can see the p-values for all predictors. Anything greater than 0.05 can be removed. We can remove: So, Po1, Po2, LF, M.F, Pop, NW, U1, U2, Wealth, and Time. We are then left with M, Ed, Ineq, and Prob.

### Create a new model with selected predictors, and use it to predict crime in new city
```{r}
crime_model_new <- lm(uscrime$Crime ~ M + Ed + Ineq + Prob, data = uscrime)

summary(crime_model_new)

predicted_crime <- predict(crime_model_new, newdata = new_city)

cat("Predicted Crime:", predicted_crime, "\n")
cat("Actual Crime Range:", range(uscrime$Crime))
```
This new model predicted a value, 897.2307, that is much more realistic than the previous prediction. However, we can see that the adjusted r-squared value dramatically decreased. This indicates that some predictors we removed were important. We need to add some back. There were some predictors that were borderline important, Po1 and U2. Let's add them back.

### Create a new model with Po1 and U2 added again, and use it to predict crime in new city
```{r}
crime_model_optimal <- lm(uscrime$Crime ~ M + Ed + Po1 + U2 + Ineq + Prob, data = uscrime)

summary(crime_model_optimal)

predicted_crime <- predict(crime_model_optimal, newdata = new_city)

cat("Predicted Crime:", predicted_crime, "\n")
cat("Actual Crime Range:", range(uscrime$Crime))
```
This new model predicted a value, 1304.245, that is still a realistic prediction. We can also see now that the r-squared value has dramatically increased, to an even higher value than the original model. The model seems like a good fit. We need to check the quality of fit.

### Check quality of fit of regression model
```{r}
cat("Original Model AIC:", AIC(crime_model), "\n")
cat("Original Model BIC:", BIC(crime_model), "\n")

cat("New Model AIC:", AIC(crime_model_new), "\n")
cat("New Model BIC:", BIC(crime_model_new), "\n")

cat("Optimal Model AIC:", AIC(crime_model_optimal), "\n")
cat("Optimal Model BIC:", BIC(crime_model_optimal), "\n")
```
These AIC/BIC models are meaningless without testing to see which is better. We will test the original model against the optimal model.

### Test AIC
```{r}
cat("AIC Relative Likelihood:", exp(AIC(crime_model) - AIC(crime_model_optimal)))
```
As you can see, the relative likelihood is 19205.25. This is much greater than 1, and implies that the optimal model is very likely to be better than the original

### Test BIC
```{r}
cat("BIC Relative Likelihood:", abs(BIC(crime_model_optimal) - BIC(crime_model)))
```
As you can see, the relative likelihood is 26.51427. This is much greater than 10, and implies that the optimal model is very likely to be better than the original.

### Quality of fit
```{r}
summary <- summary(crime_model_optimal)

cat("Adjusted R-Squared:", summary$adj.r.squared, "\n")
cat("F-Statistic:", summary$fstatistic, "\n")
cat("Residual Standard Error:", summary$sigma, "\n")
```
- The r-squared value, 0.7307463, seems to indicate that the crime can effectively be indicated by the predictors with low variance.
- The f-statistic value, 21.8071, seems to indicate that the predictors chosen are signifigant.
- The residual standard error value, 200.6899, seems to indicate that the model can fit the data.

### Conclusion
```{r}
summary

cat("Coefficients:", summary$coefficients[, 1], "\n")

cat("Factors/Predictors:", rownames(summary$coefficients), "\n")

cat("Predicted Crime for New City:", predict(crime_model_optimal, newdata = new_city), "\n")
```
Equation of line: $Crime Rate = -5040.505 + 105.0196 * M + 196.4712 * Ed + 115.0242 * Po1 + 89.36604 * U2 + 67.65322 * Ineq - 3801.836 * Prob$